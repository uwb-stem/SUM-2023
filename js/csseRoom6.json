{
    "csse": [
        {
            "time": "1:00 PM - 1:15 PM",
            "projectId": "csse-6-100",
            "title": "Automated API Validation at Starbucks ",
            "studentName": "Phung Vo ",
            "studentMajor": "CSSE",
            "projectType": "Sponsored internship or job opportunity",
            "facultyAdvisor": "Dr. Yang Peng",
            "posterLink": "posters/csse/vophung_3825501_108488914_Poster_.jpg",
            "abstract": "There are many required components to run a Starbucks store. Making sure that there's enough coffee beans, enough milk, or enough cups is obviously a must. But there is another component that most people don't think of, and that is workforce scheduling. Workforce scheduling is crucial to operate a Starbucks store efficiently and economically. A shortage of baristas during rush hours will cause a revenue loss due to customer leaving after a long waiting time. On the other hand, when there is a surplus of baristas, the company also loses money to pay for the baristas that don't necessarily need to be there. Therefore, if schedules were as optimized as possible, it would not only increase revenue but also save operational costs. At Starbucks, workforce management (WFM) is dedicated to making optimized schedules.\n\nEvery day, Starbucks WFM sends hundreds of API requests to a 3rd party database with the help of 3rd party Labor Demand API and 3rd party Earned Hours API. However, whenever the Labor Demand API and Earned Hours API have a system update or monthly maintenance, there was no way to verify that they perform the same functionalities after the changes. My project solves this problem by validating labor demand API and earned hours API functionalities.\n\nMy internship work started by manually testing the APIs with Postman and identify test functionality scenarios that can be automated. Then, I set up a local workspace where I imported libraries, such as REST Assured and TestNG to help with testing REST APIs. After designing and implementing the framework, I integrated it into Jenkins to create parameterized schedule for running it in different regions. Lastly, I integrated the job status notification to Slack. As a result of this project, the functionality validation of Labor Demand API and Earned Hours API is now automated. With my contribution, the automated process reduces the testing time of 40 API functionalities by  90%, from 15 minutes of manual work down to 1.5 minutes of computer work. Overall, this API validation allows my team to send API requests with full confidence that all API functionalities work as intended, improving the project timeline speed, and decreasing the debugging time"
        },
        {
            "time": "1:15 PM - 1:30 PM",
            "projectId": "csse-6-115",
            "title": "Aitheon",
            "studentName": "Camas Collins",
            "studentMajor": "CSSE",
            "projectType": "Sponsored internship or job opportunity",
            "facultyAdvisor": "Dr. Clark Olson",
            "posterLink": "posters/csse/collinscamas_4107623_108490222_Collins_C_Poster_8_23.jpg",
            "abstract": "Working at a startup has its ups and downs. On one hand there are limited resources and the amount of time they can afford to teach you is limited, but on the other hand the projects they give you are more impactful than any major company would ever consider assigning to an intern.\n\nWith Alitheon, this meant that I was able to work on high level code that I never thought I’d work on in an internship. Within the first week I was asked challenging questions, provided difficult assignments, and trusted to spearhead projects alone. Over the course of the internship, I was assigned two major projects:\n1. A focus assistant that will be used to help workers quickly and correctly focus industrial camera lenses and regularly check if a lens has gone out of focus all while recording data to help recognize which cameras need work.\n2. A full stack OCR engine that starts from a shell script, sets local variables, runs a Docker compose file to load both the locally hosted OCR REST server and the camera service from which the images of text are captured, and the cropping tool to help users declare that area of a specific object type that important text can be found in.\n\nBoth projects taught me so much about what it takes to be a software developer in the modern world. The lessons learned and the struggles faced when trying to overcome the challenges assigned to me were monumental towards the development that I see in myself as this internship comes to an end.\n\nThe real world is vastly different from college, and the fact that I was able to experience those differences will be some of the most useful tools I have on my belt as I leave the comforts of college to face the real world. The result of my struggles is the reward of employment with Alitheon. I have enjoyed my internship and I am ready to leave my mark in the computer vision world."
        },
        {
            "time": "1:30 PM - 1:45 PM",
            "projectId": "csse-6-130",
            "title": "Deep Tracer 3.0: A Proof of Concept",
            "studentName": "Aditya Duggirala",
            "studentMajor": "CSSE",
            "projectType": "Faculty research",
            "facultyAdvisor": "Dr. Dong Si",
            "posterLink": "posters/csse/duggiralaaditya_3935604_108494794_Capstone Poster (3).png",
            "abstract": "DeepTracer 2.0 uses a CNN model showing a 2D image of a protein structure in Euclidian space. Only image classification is possible, and nodes, edges, and graphs can’t be predicted since those parts of the protein are shown in non-euclidian space with three dimensions. This is why we want a new version called DeepTracer 3.0 which utilizes a GNN model. Proof of concept must be established to show that adding a GNN model is worth implementing in the code base for DeepTracer 2.0. With help from my mentor Jason Chen, we made a custom pipeline using the output from DeepTracer 2.0 and GNN model refinement from Model Angelo. The output from this pipeline is compared to the Model Angelo pipeline. After evaluating metrics, we can see if it’s worth moving forward with DeepTracer 3.0 or if we must make changes. If metrics show the pipeline output is the same or worse than Model Angelo’s pipeline, then an improvement must be proposed. If the results are better, then DeepTracer 3.0 can be implemented. This is done by converting the protein database file from DeepTracer 2.0 prediction into a CIF file. After this, the file path is put in the Initial C-alpha Prediction in Model Angelo’s build. This is done so Model Angelo uses Deep Tracer’s prediction. The GNN model refinement from Model Angelo’s build will use the C-Alpha prediction from DeepTracer 2.0 as the structure of the GNN output. Finally, the evaluation command will be used to compare differences between both pipelines with the input folder of a protein containing the fasta sequence, the MRC file, and the protein data bank map file. This is done by comparing how many protein structures (including secondary structures) and fasta sequences match the input folder. Which pipeline’s output is closer to matching the input folder is the more accurate between the two and determines if improvements need to be made. After gathering results, the custom pipeline matches the Model Angelo output or has higher accuracy than the Model Angelo output. If we add a similar GNN model, we will have the same results for some proteins or better outcomes for others."
        },
        {
            "time": "1:45 PM - 2:00 PM",
            "projectId": "csse-6-145",
            "title": "Writing a Script to Remove Old Files from the Production Server",
            "studentName": "Amlak Takele",
            "studentMajor": "CSSE",
            "projectType": "Faculty Research",
            "facultyAdvisor": "Dr. Dong Si ",
            "posterLink": "posters/csse/takeleamlakalegn_4076189_108490024_Capstone_poster.png",
            "abstract": "As an active participant in the advanced DeepTracer research program conducted by Data Analysis and Intelligent Systems (DAIS) research group, my capstone project mainly focused on developing an efficient script for clearing outdated files from the production server to efficiently use limited available file storage. \n\nFor the script development task, my first step was to completely understand the DeepTracer production server. Using Linux skills and knowledge gained from CSS courses, I carefully explored the directory and file structure of the DAIS 8 production server. By understanding the server's layout, I successfully identified outdated files and determined the relevant time span for their removal. Building on this foundation, I devoted considerable effort to creating, rigorously testing, and ultimately automating a Python script designed explicitly for efficiently deleting obsolete files within the specified directory. Additionally, in pursuit of the script task, I undertook the local installation of DeepTracer's frontend and backend repositories on my computer to further assist the file identification and removal process, despite encountering some challenges during the setup process. This endeavor was a valuable learning experience, as it allowed me to acquire practical skills in handling virtual environments, npm, pip installations, running the Angular client, and using MongoDB. \n\nI believe my contributions to the DeepTracer research and script development tasks have expanded my knowledge in structural biology, software engineering, and AI and added valuable input to the DAIS research group's progress. Moving forward, I am excited about further refining these contributions and delving deeper into the frontiers of innovative research."
        },
        {
            "time": "2:00 PM - 2:15 PM",
            "projectId": "csse-6-200",
            "title": "Software Engineering Internship @ Amazon",
            "studentName": "Rumaisa Rahma Nazeeruddin",
            "studentMajor": "CSSE",
            "projectType": "Sponsored internship or job opportunity",
            "facultyAdvisor": "Dr. William Erdly",
            "posterLink": "posters/csse/nazeeruddinrumaisarahma_4025434_108486731_capstone.jpg",
            "abstract":"During my Amazon internship, I collaborated with the Security team to design a dynamic data management service tailored specifically for Amazon Sites. This involved creating a comprehensive database containing essential information for each Site, complemented by a user-friendly dashboard, greatly enhancing accessibility for our valued customers. \n\nBefore the introduction of this innovative service, the data management process for each Amazon Site was reliant on a static, hardcoded list. This approach not only posed the risk of outdated information but also entailed significant development efforts for any necessary data updates. However, our solution revolutionized this process, centralizing critical data and enabling seamless, real-time updates. This transformation not only ensured data accuracy and timeliness but also markedly reduced the development burden associated with manual data maintenance. \n\nThe pivotal achievement of this service was its ability to provide Amazon with a unified and up-to-the-minute overview of each Site. By efficiently aggregating data from various sources into a single accessible location, the service streamlined the process, eliminating the need to navigate through multiple data repositories. This consolidation of data was invaluable, simplifying data management across the board and perfectly exemplifying Amazon's commitment to innovation and operational efficiency. \n\nThis project not only highlighted the power of technology in driving significant process enhancements but also showcased Amazon's dedication to staying at the forefront of digital innovation. By creating a centralized, continuously updated repository of information for Amazon Sites, our service not only ensured data accuracy but also optimized resource allocation, leading to more informed decisions and overall operational effectiveness throughout the organization. This experience underscored the transformative potential of innovative solutions in reinforcing Amazon's position as a leader in the ever-evolving digital landscape."
        },
        {
            "time": "2:15 PM - 2:30 PM",
            "projectId": "csse-6-215",
            "title": "Developer Experience at Bridge ",
            "studentName": "Kevin Yehoon Lee",
            "studentMajor": "CSSE",
            "projectType": "Sponsored internship or job opportunity",
            "facultyAdvisor": "Dr. William Erdly",
            "posterLink": "posters/csse/leekevinyehoon_3791109_108484084_CapstonePoster (KevinYLee)-1.png"
        },
        {
            "time": "2:30 PM - 2:45 PM",
            "projectId": "csse-6-230",
            "title": "Assessing QuickCheck: An In-Depth Analysis of Testing in a Vision Screening Mobile Application",
            "studentName": "Noah Nguyen",
            "studentMajor": "CSSE",
            "projectType": "Faculty research",
            "facultyAdvisor": "Dr. William Erdly",
            "posterLink": "posters/csse/Noah Nguyen.png",
            "abstract": "The Educating Young Eye (EYE) Research Group (led by Dr. William Erdly, Ph.D.), in collaboration with the Near Vision Insititute (led by Dr. Alan Pearson, OD Ph.D. FCOVD), focuses on children near vision assessment and therapy using the latest modern technologies based on existing research works such as virtual reality and mobile applications. \n\nOne of the EYE Research Group’s ongoing projects, QuickCheck, is a revolutionary project aimed at addressing undiagnosed and untreated vision problems in school-aged children. The project integrates technology into pediatric vision care, particularly with the application developed using the Unity engine. It is an application that can determine symptoms of convergence insufficiency by using the Convergence Insufficiency Symptom Survey (CISS) to help people determine whether they should see an eye doctor. To bolster the project's reliability and efficacy, my role involved load testing and user-assisted testing of the QuickCheck application. This ensured that the software application functioned optimally under different user loads. This analysis used varying durations and user quantities to emulate real-world usage patterns. \n\nResults indicated that the QuickCheck app could consistently support 300 users across all test durations, showcasing its robustness. However, a session-related challenge surfaced during extended testing, hinting at potential difficulties in session management that might affect user experience. \n\nThe QuickCheck application embodies a pivotal shift in pediatric vision care, blending advanced technology with essential medical assessments. While the preliminary results are promising, we recognize that we're in the dynamic phase of clinical trials. Gathering real-time data and insights from these trials, coupled with feedback from educators through user-assisted report forms, will be instrumental in refining the application. Our vision is not just to iterate upon the present build, but to enhance it to a level where it's ready for public deployment. This holistic approach ensures that when QuickCheck is introduced to a broader audience, it's not just technologically sound but also resonates with the genuine needs and feedback of its users."
        },
        {
            "time": "2:45 PM - 3:00 PM",
            "projectId": "csse-6-245",
            "title": "QuickCheck: Vision Screening Tool",
            "studentName": "Mahsa Mohajeri",
            "studentMajor": "CSSE",
            "projectType": "Faculty research",
            "facultyAdvisor": "Dr William Erdly",
            "posterLink": "posters/csse/mohajerimahsa_4106860_108493371_Poster.jpg",
            "abstract":"QuickCheck is an application crafted to offer schools a streamlined tool for swiftly and efficiently assessing the visual acuity of students. This versatile app encompasses evaluations of near vision acuity, distance vision acuity, stereopsis (a future module), and convergence insufficiency (via a clinical assessment survey). The assessment of near vision acuity sheds light on students' capacity to discern objects up close, a pivotal skill for reading. The examination of far vision acuity is the most familiar to users since it is typically administered during medical appointments. The Stereopsis test gauges the aptitude to discern depth in visual perception. Lastly, convergence pertains to the capability to concentrate visual focus on a specific target. These four examinations play a vital role in detecting any potential visual impairments among students. \n\nThe main objective of my capstone project revolved around preparing QuickCheck for its launch on the Google Store. I conducted a series of comprehensive functional assessments aimed at uncovering any potential usability concerns. A particular focus during these tests was directed towards confirming the responsiveness and expected outcomes of each button within the mobile application. Additionally, I evaluated the application's capacity to respond properly to a variety of potential user inputs – including proper responses, boundary testing, error/correction handling, logic/flow, and application instructions. Lastly, I conducted specific tests to validate the accurate storage of student test results in the database. \n\nA significant portion of these assessments were carried out successfully. Nonetheless, there were specific tests the application was not able to pass; these instances have been recorded and incorporated as items in the product backlog within the development environment. Importantly, unsuccessful tests do not hinder the continuation of clinical testing. My efforts have contributed to identifying usability concerns in the current version of the application, which are crucial for the timely and proper clinical launch of QuickCheck. "
        },
        {
            "time": "3:00 PM - 3:15 PM",
            "projectId": "csse-6-300",
            "title": "Fantasy Valorant",
            "studentName": "Samuel Zunja Xiao",
            "studentMajor": "CSSE",
            "projectType": "Group Project - Student Defined",
            "facultyAdvisor": "Dr. William Erdly",
            "posterLink": "posters/csse/xiaosamuelzunjia_3492800_108494041_Sam Xiao Fantasy Valorant Poster.png",
            "abstract": "Two of my favorite hobbies are competitive video games (or e-sports) and fantasy sports. A popular hobby for sports fans, fantasy sports allows enthusiasts to create a hypothetical team of all their favorite players and pit them against other fantasy teams using statistical data from the athletes’ real performances. Combining fantasy sports with e-sports is a niche that is not yet explored, despite the incredible growths of both industries in recent times. \n\nFantasy Valorant aims to utilize the cutting-edge and commonly utilized MERN (MongoDB, Express.js, React.js, Node.js) stack to build a robust, full-stack application. MongoDB provides the database that will store the users’ login information, rosters, and statistics of e-sports players and tournaments. Express.js and Node.js combine to provide the server framework and middleware tools for the application. Finally, on the front end, React.js is an expansive JavaScript library that allows for effective user interfaces. The greatest appeal of the MERN stack is arguably the fact that all the architecture components can be written in JavaScript, allowing for seamless transitions between the technologies and greatly lowering the burden of entry. For example, libraries like mongoose and Redux help to streamline database access and state management, respectively. \n\nFantasy Valorant allows users to view a plethora of data on the performances of their favorite players and teams. Users can compete with other users within their league to draft star players and see how their teams fare against other users’. Admins can also invite new users to the platform to participate. Users are authenticated and authorized with JSON web tokens, and login states are stored. This project successfully combined the appeal of managing your favorite sports players in hypothetical head-to-head competition with the burgeoning competitive e-sports domain. \n\nDeveloping Fantasy Valorant taught many valuable skills about using such a powerful and convenient technology stack. Full-stack development is very involved due to the intricate connections that must be managed between the backend database, the user experience on the frontend, and the server architecture in between. On the other hand, it was extremely difficult to learn how to manage the different technologies and integrate them together. I also gained valuable experience in working on complex software with a partner and in managing realistic expectations of project scope. Fantasy Valorant was extremely rewarding to develop and helped to develop critical skills in web development that will undoubtedly aid me in my future career."
        },
        {
            "time": "3:15 PM - 3:30 PM",
            "projectId": "csse-6-315",
            "title": "Fantasy Valorant",
            "studentName": "Irwin Li",
            "studentMajor": "CSSE",
            "projectType": "Group Project - Student Defined",
            "facultyAdvisor": "Dr. William Erdly",
            "posterLink": "posters/csse/liirwin_3477731_108494043_FantasyValorantPoster.png",
            "abstract": "The purpose of this project was to design and develop a web application that provides users with an immersive and interactive fantasy sports experience based on the rising e-sport, Valorant. The application we built allows users to build a virtual team with the goal of drafting a roster of players that exist within the current competitive circuit of Valorant esports. \n\nOur developmental process included a full-stack approach where we tackled both the front-end and back-end aspects of a website application. To accomplish this, we chose and utilized the MERN (MongoDB, Express.js, React.js, and Node.js) stack to build our application. The back-end consists of Express.js and Node.js, which provided a way to retrieve and manipulate data efficiently from our database, MongoDB. MongoDB stores user information as well as the player information we wanted to display using Valorant esports events. Additionally, we utilized Mongoose, an Object Data Modeling library that helps manage our data. Our front-end was developed using React.js, which helped provide an easy way to create intuitive and interactive user interfaces that helps enhance the user experience by providing smooth functionalities and tools for our users. We also used Redux and RTK query as a tool to fetch data and manage user states within React.js. \n\nWith our Fantasy Valorant web application project, we wanted to provide a platform for a rising competitive community within esports - a platform that could ultimately enhance and integrate a new way of enjoying the competitive Valorant scene. To do that, we had to make sure we used the right tools and had the right approach to provide an interactive and responsive experience that would satisfy the expectations of our audience. While the scope of our project was extensive and thus created many challenges throughout the developmental process, it was ultimately an extremely fulfilling project that taught me how to collaborate and work together with a partner. "
        }
    ]
}

